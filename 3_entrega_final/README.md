# Entrega 4: Algoritmos de RL Modernos con Deep Learning y Curriculum Learning

## ğŸ“‹ Cambios Principales respecto a Entrega 3

### 1. **Algoritmos Modernos (2015-2023)**
- âœ… **PPO (Proximal Policy Optimization, 2017)** - Estado del arte en RL
- âœ… **TRPO (Trust Region Policy Optimization, 2015)** - Predecesor de PPO, optimizaciÃ³n con restricciÃ³n de confianza
- âœ… **DQN (Deep Q-Network, 2015)** - Q-Learning con redes neuronales profundas
- âœ… **A2C (Advantage Actor-Critic, 2016)** - MÃ©todo actor-crÃ­tico asÃ­ncrono
- âŒ ~~Q-Learning tabular (1989)~~ - Obsoleto
- âŒ ~~SARSA tabular (1994)~~ - Obsoleto
- âŒ ~~Monte Carlo (1940s-1950s)~~ - Demasiado antiguo

### 2. **Deep Learning Integration**
- **Redes Neuronales Profundas**: Todos los algoritmos usan PyTorch/TensorFlow
- **Feature Extraction**: CNN para procesar observaciones visuales
- **Arquitecturas modernas**: MLP para estados, CNN para imÃ¡genes
- **LibrerÃ­as confiables**: Stable-Baselines3 (implementaciÃ³n verificada y optimizada)

### 3. **Curriculum Learning - ProgresiÃ³n de Herramientas**
Entrenamiento progresivo en 4 etapas con **transfer learning** (cada stage usa el modelo pre-entrenado del anterior):

#### **Stage 1: Madera (~500 episodios)**
- **Objetivo**: Recolectar 3 madera â†’ Craftear `wooden_pickaxe`
- **Arena**: 10Ã—10 con alta densidad de Ã¡rboles (40-60 logs)
- **Inventario inicial**: planks + sticks (para crafteo)
- **PropÃ³sito**: Aprender movimiento, ataque, y concepto de recolecciÃ³n
- **Success threshold**: 60% (avanza cuando alcanza este %)

#### **Stage 2: Piedra (~500 episodios)**
- **Objetivo**: Recolectar 3 stone â†’ Craftear `stone_pickaxe`
- **Arena**: 10Ã—10 con alta densidad de piedra (30-40 stone)
- **Inventario inicial**: `wooden_pickaxe` + materiales para crafting
- **Pre-requisito**: Tener wooden_pickaxe para craftear
- **Success threshold**: 55%
- **Transfer learning**: Carga modelo entrenado de Stage 1

#### **Stage 3: Hierro (~600 episodios)**
- **Objetivo**: Recolectar 3 iron_ore â†’ Craftear `iron_pickaxe`
- **Arena**: 10Ã—10 con alta densidad de hierro (20-30 iron_ore)
- **Inventario inicial**: `stone_pickaxe` + materiales
- **Pre-requisito**: Tener stone_pickaxe para craftear
- **Success threshold**: 50% (mÃ¡s difÃ­cil)
- **Transfer learning**: Carga modelo entrenado de Stage 2

#### **Stage 4: Diamante (~800 episodios)**
- **Objetivo**: Recolectar 1 diamond â†’ Craftear `diamond_pickaxe`
- **Arena**: 10Ã—10 con baja densidad de diamantes (3-6 diamond_ore)
- **Inventario inicial**: `iron_pickaxe` + materiales
- **Pre-requisito**: Tener iron_pickaxe para craftear
- **Success threshold**: 45% (muy difÃ­cil)
- **Transfer learning**: Carga modelo entrenado de Stage 3

**CaracterÃ­sticas clave**:
- âœ… **Auto-crafteo**: Cuando alcanza materiales requeridos, craftea automÃ¡ticamente
- âœ… **Pitch auto-reset**: Si mira arriba/abajo >10s â†’ reset a 0Â° con penalizaciÃ³n -300
- âœ… **Rewards escalados**: Aumentan con dificultad de stage (500â†’1000â†’1500â†’2000)
- âœ… **SimpleCraftCommands**: XML configurado para crafteo automÃ¡tico de herramientas
- âœ… **DetecciÃ³n de Ã©xito**: Episodio termina al craftear la herramienta objetivo

### 4. **Arquitectura de Redes Neuronales**

#### **PPO, TRPO & A2C (Actor-Critic)**
```
Actor Network:
  Input: Observation (state vector)
  â†’ Dense(64, ReLU)
  â†’ Dense(64, ReLU)
  â†’ Output: Action probabilities (softmax)

Critic Network:
  Input: Observation
  â†’ Dense(64, ReLU)
  â†’ Dense(64, ReLU)
  â†’ Output: Value estimate (scalar)
```

#### **DQN (Q-Network)**
```
Q-Network:
  Input: Observation
  â†’ Dense(128, ReLU)
  â†’ Dense(128, ReLU)
  â†’ Dense(64, ReLU)
  â†’ Output: Q-values para cada acciÃ³n
```

### 5. **JustificaciÃ³n TÃ©cnica**

#### **Â¿Por quÃ© PPO?**
- **Paper**: "Proximal Policy Optimization Algorithms" (Schulman et al., 2017)
- **Ventajas**: 
  - Estable y robusto
  - No requiere ajuste fino de hiperparÃ¡metros
  - Estado del arte en robÃ³tica y juegos
- **Uso real**: OpenAI Five (Dota 2), OpenAI Rubik's Cube

#### **Â¿Por quÃ© TRPO?**
- **Paper**: "Trust Region Policy Optimization" (Schulman et al., 2015)
- **Ventajas**:
  - GarantÃ­as teÃ³ricas de mejora monÃ³tona
  - RestricciÃ³n de regiÃ³n de confianza (KL divergence)
  - Predecesor directo de PPO
  - MÃ¡s estable que mÃ©todos vanilla policy gradient
- **Uso real**: RobÃ³tica de alta precisiÃ³n, tareas de manipulaciÃ³n

#### **Â¿QuÃ© modelo de Deep Learning usa?**
- **Optimizador**: Adam (learning rate: 3e-4)
- **Arquitectura**: Multi-Layer Perceptron (MLP) con 2 capas ocultas
- **FunciÃ³n de activaciÃ³n**: ReLU
- **NormalizaciÃ³n**: Layer Normalization
- **RegularizaciÃ³n**: Gradient clipping (0.5)

#### **Â¿CÃ³mo se entrena la Q en DQN?**
```python
# Red neuronal aproxima Q(s,a)
Q_predicted = q_network(state)[action]

# Target usando red objetivo (frozen)
Q_target = reward + gamma * max(target_network(next_state))

# Loss: Mean Squared Error
loss = MSE(Q_predicted, Q_target)

# Backpropagation con Adam optimizer
optimizer.zero_grad()
loss.backward()
optimizer.step()

# Actualizar target network cada N pasos
if steps % target_update_freq == 0:
    target_network.load_state_dict(q_network.state_dict())
```

## ğŸ“ Estructura del Proyecto

```
4_entrega/
â”œâ”€â”€ README.md                          # Este archivo
â”œâ”€â”€ requirements.txt                   # Dependencias (Stable-Baselines3, PyTorch, etc.)
â”œâ”€â”€ setup.sh                          # Script de instalaciÃ³n
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ malmo_env_wrapper.py          # Wrapper Gym para Malmo
â”‚   â”œâ”€â”€ curriculum_manager.py         # Gestor de curriculum learning
â”‚   â”œâ”€â”€ feature_extractor.py          # ExtracciÃ³n de caracterÃ­sticas
â”‚   â”œâ”€â”€ custom_policies.py            # PolÃ­ticas personalizadas para SB3
â”‚   â””â”€â”€ utils.py                      # Utilidades
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ ppo_config.yaml               # ConfiguraciÃ³n PPO
â”‚   â”œâ”€â”€ trpo_config.yaml              # ConfiguraciÃ³n TRPO
â”‚   â”œâ”€â”€ dqn_config.yaml               # ConfiguraciÃ³n DQN
â”‚   â”œâ”€â”€ a2c_config.yaml               # ConfiguraciÃ³n A2C
â”‚   â””â”€â”€ curriculum_stages.yaml        # DefiniciÃ³n de etapas
â”‚
â”œâ”€â”€ train_ppo.py                      # Script de entrenamiento PPO
â”œâ”€â”€ train_trpo.py                     # Script de entrenamiento TRPO
â”œâ”€â”€ train_dqn.py                      # Script de entrenamiento DQN
â”œâ”€â”€ train_a2c.py                      # Script de entrenamiento A2C
â”œâ”€â”€ train_curriculum.py               # Script con curriculum learning
â”‚
â”œâ”€â”€ evaluate.py                       # EvaluaciÃ³n de modelos
â”œâ”€â”€ compare_algorithms.py             # ComparaciÃ³n entre algoritmos
â”œâ”€â”€ visualize_training.py             # VisualizaciÃ³n de mÃ©tricas
â”‚
â”œâ”€â”€ models/                           # Modelos entrenados guardados
â”œâ”€â”€ logs/                            # TensorBoard logs
â””â”€â”€ results/                         # Resultados y grÃ¡ficos
```

## ğŸš€ InstalaciÃ³n

### Requisitos
- Python 3.8+
- PyTorch 1.10+
- CUDA (opcional, para GPU)

### Setup
```bash
cd 4_entrega
pip install -r requirements.txt
```

## ğŸ’» Uso

### OpciÃ³n A: Experimento Completo Automatizado (Recomendado) ğŸš€

El script `run_full_experiment` entrena todos los algoritmos en paralelo, evalÃºa y compara automÃ¡ticamente.

#### **Windows (PowerShell)**
```powershell
# Testing rÃ¡pido (50 episodios, ~1-2 horas)
.\run_full_experiment.ps1 -Mode fast

# ProducciÃ³n completa (3000 episodios, ~24-48 horas)
.\run_full_experiment.ps1 -Mode full
```

#### **Linux/WSL (Bash)**
```bash
# Testing rÃ¡pido
chmod +x run_full_experiment.sh
./run_full_experiment.sh fast

# ProducciÃ³n completa
./run_full_experiment.sh full
```

**El script hace todo automÃ¡ticamente:**
1. âœ… Entrena PPO, TRPO, DQN, A2C en **paralelo** (puertos distintos: 10000, 10003, 10001, 10002)
2. âœ… Espera a que todos terminen
3. âœ… EvalÃºa cada modelo en los 4 stages
4. âœ… Genera comparaciÃ³n con grÃ¡ficos
5. âœ… Guarda todo en `results/experiment_TIMESTAMP/`

**Output generado:**
```
results/experiment_TIMESTAMP/
  â”œâ”€â”€ ppo_evaluation.json
  â”œâ”€â”€ trpo_evaluation.json
  â”œâ”€â”€ dqn_evaluation.json
  â”œâ”€â”€ a2c_evaluation.json
  â”œâ”€â”€ comparison_results_*.json
  â””â”€â”€ algorithm_comparison_*.png  (grÃ¡fico comparativo)

logs/experiment_TIMESTAMP/
  â”œâ”€â”€ ppo_training.log
  â”œâ”€â”€ trpo_training.log
  â”œâ”€â”€ dqn_training.log
  â”œâ”€â”€ a2c_training.log
  â””â”€â”€ comparison.log
```

---

### OpciÃ³n B: Entrenamiento Manual Individual

### 1. Entrenamiento con PPO (Recomendado)
```bash
# Testing rÃ¡pido (30 episodios por stage, avanza con 30% completado)
python train_ppo.py --episodes 50 --curriculum

# ProducciÃ³n (cambiar episodes_per_stage a 500 en curriculum_manager.py)
python train_ppo.py --episodes 5000 --curriculum
```

### 2. Entrenamiento con TRPO
```bash
# Testing rÃ¡pido
python train_trpo.py --episodes 50 --curriculum

# ProducciÃ³n
python train_trpo.py --episodes 5000 --curriculum
```

### 3. Entrenamiento con DQN
```bash
# Testing rÃ¡pido
python train_dqn.py --episodes 50 --curriculum

# ProducciÃ³n
python train_dqn.py --episodes 5000 --curriculum
```

### 4. Entrenamiento con A2C
```bash
# Testing rÃ¡pido
python train_a2c.py --episodes 50 --curriculum

# ProducciÃ³n
python train_a2c.py --episodes 5000 --curriculum
```

### 5. EvaluaciÃ³n
```bash
# Evaluar un modelo en un stage especÃ­fico
python evaluate.py --algorithm ppo --model models/ppo_curriculum_*_final.zip --stage 1 --episodes 10

# Evaluar en todos los stages
python evaluate.py --algorithm ppo --model models/ppo_curriculum_*_final.zip --episodes 10
```

### 6. ComparaciÃ³n de Algoritmos
```bash
# Comparar PPO, TRPO, DQN y A2C en todos los stages
python compare_algorithms.py \
  --models models/ppo_curriculum_*_final.zip models/trpo_curriculum_*_final.zip models/dqn_curriculum_*_final.zip models/a2c_curriculum_*_final.zip \
  --algorithms ppo trpo dqn a2c \
  --episodes 10 \
  --stages 1 2 3 4
```

**Nota**: Por defecto, el curriculum usa 30 episodios por stage para testing rÃ¡pido. Para entrenamiento completo, editar `src/curriculum_manager.py` y cambiar `episodes_per_stage` de 30 a 500-800.

## ğŸ“Š MÃ©tricas y EvaluaciÃ³n

### MÃ©tricas Registradas
- **Reward por episodio**: Recompensa acumulada
- **Success rate**: % episodios con objetivo cumplido
- **Episode length**: Pasos por episodio
- **Loss**: PÃ©rdida de la red neuronal
- **Learning rate**: Tasa de aprendizaje adaptativa
- **Entropy**: ExploraciÃ³n vs explotaciÃ³n
- **Value loss**: Error en estimaciÃ³n de valor (A2C/PPO)
- **Policy loss**: Error en polÃ­tica (A2C/PPO)

### VisualizaciÃ³n
```bash
# TensorBoard
tensorboard --logdir logs/

# GrÃ¡ficos personalizados
python visualize_training.py --log_dir logs/PPO_1
```

## ğŸ”¬ Fundamento TeÃ³rico

### PPO (Proximal Policy Optimization)
- **Paper**: [Schulman et al., 2017](https://arxiv.org/abs/1707.06347)
- **Tipo**: Policy Gradient con restricciÃ³n de confianza
- **Ventaja**: Estable, sample-efficient, fÃ¡cil de ajustar

### TRPO (Trust Region Policy Optimization)
- **Paper**: [Schulman et al., 2015](https://arxiv.org/abs/1502.05477)
- **Tipo**: Policy Gradient con regiÃ³n de confianza (KL divergence)
- **Ventaja**: GarantÃ­as teÃ³ricas de mejora monÃ³tona, mÃ¡s estable que vanilla policy gradient

### DQN (Deep Q-Network)
- **Paper**: [Mnih et al., 2015](https://www.nature.com/articles/nature14236)
- **Tipo**: Value-based con replay buffer y target network
- **Ventaja**: Aprende directamente de pÃ­xeles

### A2C (Advantage Actor-Critic)
- **Paper**: [Mnih et al., 2016](https://arxiv.org/abs/1602.01783)
- **Tipo**: Actor-Critic con ventaja normalizada
- **Ventaja**: Balancea exploraciÃ³n/explotaciÃ³n

### Curriculum Learning
- **Paper**: [Bengio et al., 2009](https://ronan.collobert.com/pub/matos/2009_curriculum_icml.pdf)
- **ImplementaciÃ³n**: ProgresiÃ³n de herramientas (madera â†’ piedra â†’ hierro â†’ diamante)
- **CaracterÃ­stica**: Cada stage usa modelo pre-entrenado del anterior
- **LÃ³gica**: Auto-crafteo al alcanzar materiales, pitch auto-reset con penalizaciÃ³n -300

## ğŸ“š Referencias

1. **Stable-Baselines3**: https://stable-baselines3.readthedocs.io/
2. **OpenAI Spinning Up**: https://spinningup.openai.com/
3. **PyTorch RL Tutorials**: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html
4. **Malmo Platform**: https://github.com/microsoft/malmo

