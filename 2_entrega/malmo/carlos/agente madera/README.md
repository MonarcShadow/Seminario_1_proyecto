# Agente RL - Recolecci√≥n de Madera en Minecraft

Este agente de Aprendizaje por Refuerzo (Q-Learning) aprende a buscar, picar y recolectar madera en Minecraft usando Malmo.

## üéØ Objetivo

El agente debe:
1. **Buscar** √°rboles en el mundo
2. **Acercarse** a los bloques de madera
3. **Picar** el bloque hasta romperlo
4. **Recoger** el drop acerc√°ndose al item para agregarlo al inventario

## üìã Requisitos

- Minecraft 1.11.2
- Malmo instalado y configurado
- Python 3.x
- Bibliotecas: numpy, matplotlib, pickle

## üöÄ Uso

### 1. Verificar que funciona (Mundo Plano)

**RECOMENDADO PRIMERO**: Prueba en mundo plano para evitar problemas de terreno

```bash
# Prueba r√°pida de movimientos (30 segundos)
python test_movimiento.py

# Entrenamiento de prueba en mundo plano (10 episodios)
python entrenar_plano.py 10
```

El mundo plano evita:
- ‚ùå Muerte por ca√≠da
- ‚ùå Sofocaci√≥n en bloques
- ‚ùå Spawn en agua/lava
- ‚ùå Terreno irregular

### 2. Entrenar en mundo normal

Una vez verificado que funciona en mundo plano:

```bash
python mundo_rl.py
```

El entrenamiento ejecutar√° 50 episodios por defecto. Cada episodio:
- Dura m√°ximo 120 segundos o 800 pasos
- El agente spawn en posiciones aleatorias
- Usa semilla fija del archivo `.config`
- Termina cuando obtiene madera o se acaba el tiempo

### Par√°metros configurables

En `mundo_rl.py` puedes modificar:

```python
NUM_EPISODIOS = 50        # Cantidad de episodios
MODELO_PATH = "modelo_agente_madera.pkl"  # Archivo del modelo
```

En `agente_rl.py` puedes ajustar hiperpar√°metros:

```python
agente = AgenteQLearning(
    alpha=0.1,           # Tasa de aprendizaje
    gamma=0.95,          # Factor de descuento
    epsilon=0.4,         # Exploraci√≥n inicial
    epsilon_decay=0.995  # Decaimiento de exploraci√≥n
)
```

## üìä An√°lisis de Resultados

El archivo `utils.py` proporciona herramientas para analizar el entrenamiento:

```bash
# Ver resumen del entrenamiento
python utils.py resumen

# Graficar evoluci√≥n del aprendizaje
python utils.py graficar

# Analizar tabla Q aprendida
python utils.py analizar

# Exportar pol√≠tica a archivo de texto
python utils.py exportar

# Ejecutar todos los an√°lisis
python utils.py todo
```

## üéÆ Acciones Disponibles

El agente puede ejecutar 5 acciones:

1. `move 1` - Avanzar hacia adelante
2. `turn 1` - Girar 90¬∞ a la derecha
3. `turn -1` - Girar 90¬∞ a la izquierda
4. `jumpmove 1` - Saltar y avanzar (para superar obst√°culos)
5. `attack 1` - Picar bloque (mantiene presionado para romper)

## üß† Representaci√≥n del Estado

El estado se discretiza en una tupla de 9 elementos:

```python
(orientaci√≥n, madera_cerca, madera_frente, distancia_madera,
 obstaculo_frente, aire_frente, tiene_madera, altura, mirando_madera)
```

Donde:
- **orientaci√≥n**: 0=Norte, 1=Este, 2=Sur, 3=Oeste
- **madera_cerca**: 1 si detecta madera en rejilla 5x3x5, 0 si no
- **madera_frente**: 1 si hay madera justo enfrente, 0 si no
- **distancia_madera**: 0=muy cerca (puede picar), 1=cerca, 2=lejos, 3=no visible
- **obstaculo_frente**: 1 si hay obst√°culo s√≥lido, 0 si no
- **aire_frente**: 1 si hay aire enfrente, 0 si no
- **tiene_madera**: 1 si ya tiene madera en inventario, 0 si no
- **altura**: 0=bajo (<60), 1=medio (60-70), 2=alto (>70)
- **mirando_madera**: 1 si LineOfSight apunta a madera, 0 si no

## üí∞ Sistema de Recompensas

### Recompensas Positivas
- **+200**: Obtener madera en inventario (OBJETIVO)
- **+50**: Picar bloque exitosamente (de Malmo)
- **+30**: Picar cuando hay madera enfrente
- **+20**: Detectar madera muy cerca
- **+15**: Acercarse a madera visible
- **+5**: Intentar moverse despu√©s de girar
- **+3**: Movimiento exitoso
- **+2**: Mirar hacia madera

### Penalizaciones
- **-0.5**: Costo por cada acci√≥n
- **-5**: Colisi√≥n con obst√°culo
- **-10**: Picar sin madera enfrente
- **-15**: Alejarse de madera una vez detectada
- **-20**: Loop de giros detectado
- **-30**: Atascado completamente (>8 pasos sin movimiento)

## üîß Caracter√≠sticas Especiales

### Sistema Anti-Stuck
Si el agente se queda atascado sin moverse por m√°s de 12 pasos:
- Ejecuta secuencia de escape: girar y saltar
- Resetea contador despu√©s de la secuencia

### Heur√≠stica de Picado
Si el agente ve madera enfrente y est√° mir√°ndola:
- Autom√°ticamente ejecuta `attack` para picar
- Mantiene el comando por 0.5 segundos
- Realiza 3 ataques consecutivos (necesario en Minecraft 1.11.2)

### Tipos de Madera Detectados
El agente reconoce todas las variantes de madera:
- `log` - Roble, abedul, abeto, jungla
- `log2` - Acacia, roble oscuro
- `planks` - Tablas (por si acaso)

## üìà Progreso Esperado

### Primeros 10 episodios
- Alta exploraci√≥n (epsilon ~0.4)
- Aprendiendo movimientos b√°sicos
- Descubriendo el entorno

### Episodios 10-30
- Reducci√≥n de exploraci√≥n
- Comenzando a reconocer √°rboles
- Primeros intentos de picar

### Episodios 30-50
- Comportamiento m√°s dirigido
- Mayor tasa de √©xito
- Optimizaci√≥n de pasos

## üîÑ Pr√≥ximos Pasos

Este agente es el primer paso de una secuencia progresiva:

1. **Madera** ‚úÖ (actual)
2. **Piedra** (siguiente)
3. **Hierro** (futuro)
4. **Diamante** (objetivo final)

Cada etapa construye sobre la anterior, incrementando la complejidad de la tarea.

## üêõ Troubleshooting

### El agente no encuentra √°rboles
- Aseg√∫rate de que el mundo generado tenga √°rboles (bioma adecuado)
- Aumenta el n√∫mero de episodios para mejor exploraci√≥n
- Ajusta `spawn_x` y `spawn_z` para aparecer cerca de bosques

### El agente pica pero no recoge la madera
- Verifica que los items droppeados no desaparezcan (check game rules)
- El agente debe estar cerca del item para recogerlo autom√°ticamente
- Revisa que no haya obst√°culos entre el agente y el drop

### Bajo rendimiento de aprendizaje
- Aumenta `alpha` (tasa de aprendizaje) a 0.15-0.2
- Reduce `epsilon_decay` para mantener m√°s exploraci√≥n
- Incrementa `NUM_EPISODIOS` a 100+

### Errores de conexi√≥n con Malmo
- Verifica que Minecraft est√© corriendo con Malmo en puerto 10001
- Comprueba que no haya otras instancias del agente corriendo
- Reinicia Minecraft si hay problemas persistentes

## üìù Notas

- El modelo se guarda autom√°ticamente cada 10 episodios
- Puedes interrumpir el entrenamiento con Ctrl+C y el progreso se guardar√°
- La tabla Q se carga autom√°ticamente al reiniciar el entrenamiento
- Los archivos generados: `modelo_agente_madera.pkl`, `analisis_entrenamiento_madera.png`

## üë®‚Äçüíª Autor

Sistema de IA - Seminario 1 Proyecto
