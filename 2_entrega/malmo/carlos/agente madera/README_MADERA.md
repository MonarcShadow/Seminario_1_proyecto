# ğŸª“ Agente RL para RecolecciÃ³n de Madera en Minecraft

Sistema de Aprendizaje por Refuerzo (Q-Learning) para entrenar un agente que aprenda a recolectar madera en Minecraft usando Project Malmo.

## ğŸ¯ Objetivo

Entrenar un agente que pueda:
1. **Buscar Ã¡rboles** en el entorno
2. **Acercarse** a los troncos de madera
3. **Picar** los bloques usando el hacha
4. **Recolectar 3 bloques de madera** en el inventario

Este es el **primer objetivo** de una secuencia de tareas de recolecciÃ³n:
- âœ… **Madera** (actual) - 3 bloques
- ğŸ”œ Piedra
- ğŸ”œ Hierro
- ğŸ”œ Diamante

## ğŸ“ Estructura del Proyecto

```
carlos/
â”œâ”€â”€ mundo2v2.py              # Script principal de entrenamiento
â”œâ”€â”€ agente_madera_rl.py      # Agente Q-Learning
â”œâ”€â”€ entorno_madera.py        # Wrapper del entorno Malmo
â”œâ”€â”€ utils_madera.py          # Utilidades de visualizaciÃ³n
â”œâ”€â”€ modelo_agente_madera.pkl # Modelo entrenado (generado)
â””â”€â”€ README_MADERA.md         # Este archivo
```

## ğŸš€ Requisitos

### Software necesario:
- Python 3.7+
- Project Malmo instalado y configurado
- Minecraft 1.11.2 (para Malmo)
- LibrerÃ­as Python:
  ```bash
  pip install numpy matplotlib pickle
  ```

### Verificar instalaciÃ³n de Malmo:
```bash
# Debe poder importar sin errores
python -c "import MalmoPython; print('Malmo OK')"
```

## ğŸ® Uso

### 1. Entrenar el agente

```bash
# Iniciar Minecraft con Malmo en puerto 10000
# Luego ejecutar:
python mundo2v2.py
```

**ParÃ¡metros configurables** (editar en `mundo2v2.py`):
```python
NUM_EPISODIOS = 30        # Cantidad de episodios de entrenamiento
MODELO_PATH = "modelo_agente_madera.pkl"  # Ruta del modelo
```

### 2. Visualizar resultados del entrenamiento

```bash
# Generar grÃ¡ficos
python utils_madera.py graficar

# Analizar tabla Q
python utils_madera.py analizar
```

### 3. Continuar entrenamiento previo

El sistema automÃ¡ticamente carga el modelo si existe:
```python
# Si encuentra modelo_agente_madera.pkl, continÃºa desde ahÃ­
# Si no, inicia desde cero
```

## ğŸ§  CÃ³mo Funciona

### Estado del Agente

El estado se representa como una tupla de 7 elementos:

```python
estado = (
    orientacion,        # 0-3 (N, E, S, O)
    nivel_madera,       # 0=ninguna, 1=poca, 2=mucha visible
    nivel_inventario,   # 0, 1, 2, o 3+ bloques
    mirando_madera,     # 0=no, 1=sÃ­
    dist_categoria,     # 0=muy cerca, 1=cerca, 2=lejos
    obstaculo_frente,   # 0=libre, 1=bloqueado
    indicador_hojas     # 0=sin hojas, 1=algunas, 2=muchas
)
```

### Acciones Disponibles

```python
0: "move 1"        # Avanzar
1: "turn 1"        # Girar derecha 90Â°
2: "turn -1"       # Girar izquierda 90Â°
3: "jumpmove 1"    # Saltar y avanzar
4: "attack 1"      # Picar/Atacar
5: "strafe 1"      # Moverse lateral derecha
6: "strafe -1"     # Moverse lateral izquierda
```

### Sistema de Recompensas

| Evento | Recompensa | DescripciÃ³n |
|--------|-----------|-------------|
| ğŸ‰ Conseguir 3 maderas | **+500** | Â¡Objetivo completado! |
| ğŸªµ Conseguir 1 madera | **+100** | Progreso hacia objetivo |
| ğŸª“ Picar madera | **+30** | AcciÃ³n correcta |
| ğŸ‘ï¸ Mirar madera | **+20** | PreparaciÃ³n para picar |
| ğŸŒ³ Detectar madera | **+10** | Proximidad a objetivo |
| ğŸƒ Detectar hojas | **+5** | Indicador de Ã¡rbol |
| ğŸš¶ Moverse | **+3** | ExploraciÃ³n |
| âš¡ Cada acciÃ³n | **-0.5** | Costo de tiempo |
| âŒ Picar aire | **-5** | AcciÃ³n ineficiente |
| ğŸš§ ColisiÃ³n | **-10** | ObstÃ¡culo |
| ğŸ”„ Loop detectado | **-20** | Comportamiento repetitivo |
| ğŸ›‘ Atascado | **-30** | Sin progreso |

### Algoritmo Q-Learning

```
Q(s,a) â† Q(s,a) + Î±[r + Î³Â·max Q(s',a') - Q(s,a)]

Donde:
- Î± (alpha) = 0.15   : Tasa de aprendizaje
- Î³ (gamma) = 0.95   : Factor de descuento
- Îµ (epsilon) = 0.4  : ExploraciÃ³n inicial (decae a 0.05)
```

## ğŸ“Š MÃ©tricas y EvaluaciÃ³n

### Durante el entrenamiento se registra:
- âœ… **Tasa de Ã©xito**: % de episodios con 3+ maderas
- ğŸ“ˆ **Recompensa promedio**: Tendencia de aprendizaje
- ğŸ“‰ **Pasos por episodio**: Eficiencia del agente
- ğŸªµ **Madera promedio**: Progreso hacia objetivo
- ğŸ” **Epsilon (exploraciÃ³n)**: Decaimiento de exploraciÃ³n

### GrÃ¡ficos generados:
1. **EvoluciÃ³n de recompensas** (con media mÃ³vil)
2. **Madera recolectada** (verde = Ã©xito, rojo = fallo)
3. **Eficiencia** (nÃºmero de pasos)
4. **Decaimiento de exploraciÃ³n** (epsilon)

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Ajustar hiperparÃ¡metros del agente:

Editar en `mundo2v2.py`:

```python
agente = AgenteMaderaQLearning(
    alpha=0.15,         # Mayor = aprende mÃ¡s rÃ¡pido
    gamma=0.95,         # Mayor = mÃ¡s peso al futuro
    epsilon=0.4,        # Mayor = mÃ¡s exploraciÃ³n
    epsilon_decay=0.995 # Mayor = decae mÃ¡s lento
)
```

### Modificar el mundo (XML):

Editar funciÃ³n `obtener_mision_xml()` en `mundo2v2.py`:

```python
# Cambiar semilla del mundo
seed = 42  # Diferentes valores = diferentes mundos

# Cambiar tiempo lÃ­mite
timeLimitMs="120000"  # 2 minutos

# Cambiar spawn
<Placement x="0" y="64" z="0" pitch="0" yaw="0"/>
```

## ğŸ› SoluciÃ³n de Problemas

### Error: "No module named MalmoPython"
```bash
# Verificar que Malmo estÃ© instalado y en PYTHONPATH
export PYTHONPATH=$PYTHONPATH:/ruta/a/MalmoPlatform/Python_Examples
```

### Error: "Connection refused" al iniciar misiÃ³n
```bash
# Asegurarse que Minecraft con Malmo estÃ© corriendo
# Verificar que use el puerto 10000
```

### El agente no aprende / Se queda atascado
- Aumentar `epsilon` (mÃ¡s exploraciÃ³n)
- Reducir `alpha` (aprendizaje mÃ¡s conservador)
- Aumentar nÃºmero de episodios
- Verificar que haya Ã¡rboles cerca del spawn

### Recompensas muy negativas
- Revisar sistema de recompensas en `entorno_madera.py`
- Aumentar bonificaciÃ³n por detectar madera
- Reducir penalizaciÃ³n por atascarse

## ğŸ“š PrÃ³ximos Pasos

### Para extender el sistema a otros materiales:

1. **Piedra**: 
   - Cambiar `tipos_madera` por `tipos_piedra`
   - Ajustar recompensas para detectar piedra
   - Inventario inicial: pico de madera

2. **Hierro**:
   - Requiere pico de piedra
   - Buscar en cuevas/profundidad
   - Estado debe incluir nivel Y

3. **Diamante**:
   - Requiere pico de hierro
   - Buscar en Y < 16
   - Mayor dificultad de exploraciÃ³n

## ğŸ“– Referencias

- [Project Malmo Documentation](https://microsoft.github.io/malmo/)
- [Q-Learning Algorithm](https://en.wikipedia.org/wiki/Q-learning)
- [Minecraft Wiki - Wood](https://minecraft.gamepedia.com/Wood)

## ğŸ‘¥ Autor

Sistema de IA - Seminario 1 Proyecto

## ğŸ“„ Licencia

Proyecto acadÃ©mico - Universidad

---

**Â¡Buena suerte entrenando tu agente! ğŸ¤–ğŸª“ğŸŒ³**
