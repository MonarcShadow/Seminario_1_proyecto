# Agente RL Progresivo - Resumen Ejecutivo

## ðŸŽ¯ Objetivo

Crear un agente de Aprendizaje por Refuerzo que progrese a travÃ©s de mÃºltiples objetivos en Minecraft, siguiendo la cadena tecnolÃ³gica del juego:

**Madera â†’ Piedra â†’ Hierro â†’ Diamante**

## ðŸ“‹ CaracterÃ­sticas Principales

### ProgresiÃ³n JerÃ¡rquica
- **4 Fases secuenciales** que deben completarse en orden
- **Crafteo simulado** de herramientas al completar cada fase
- **Castigos** por intentar saltar fases o usar herramienta incorrecta

### Entorno Controlado
- **Mundo plano 50Ã—50** con materiales distribuidos aleatoriamente
- **Muro de obsidiana** perimetral para limitar exploraciÃ³n
- **Spawn fijo** en el centro (0, 4, 0)
- **Sin mobs, sin hambre** para enfocarse en el objetivo

### Algoritmo: Q-Learning Modular
- **Q-tables separadas por fase** (mejor especializaciÃ³n)
- **ParÃ¡metros adaptativos** (Î±, Îµ) segÃºn complejidad de fase
- **12 dimensiones de estado** (orientaciÃ³n, proximidad, herramienta, etc.)
- **7 acciones** (movimiento + giro + salto + ataque + pitch)

### Sistema de Recompensas Inteligente
- **Recompensas escaladas** segÃºn fase (Ã—1.0 a Ã—2.0)
- **Castigo por herramienta incorrecta** (-40 a -100)
- **Castigo por holgazanerÃ­a** (cerca sin picar)
- **Bonus por exploraciÃ³n vertical** (pitch)

## ðŸ“Š Resultados Esperados

### MÃ©tricas de Ã‰xito
- **Tasa de Ã©xito >70%** en Ãºltimos 20 episodios
- **Fase 3 (diamante) alcanzada** consistentemente
- **Recompensa media >1000** puntos
- **Pasos medio <500** (eficiencia)

### Curva de Aprendizaje TÃ­pica
```
Episodios 1-20:   ExploraciÃ³n, completa fase 0-1
Episodios 20-50:  Aprende fase 2, ocasionalmente fase 3
Episodios 50-100: Consistentemente completa las 4 fases
```

## ðŸš€ Inicio RÃ¡pido

### 1. Verificar Sistema
```bash
python3 test_sistema.py
```

### 2. Entrenar (prueba corta)
```bash
python3 mundo_rl.py 10 123456
```

### 3. Entrenar (completo)
```bash
python3 mundo_rl.py 100 123456
```

### 4. Ejecutar Modelo Entrenado
```bash
python3 ejecutar_modelo.py 5 123456
```

## ðŸ“ Archivos Principales

| Archivo | DescripciÃ³n |
|---------|-------------|
| `agente_rl.py` | Agente Q-Learning con 4 Q-tables modulares |
| `entorno_malmo.py` | Sistema de recompensas adaptativo |
| `mundo_rl.py` | GeneraciÃ³n mundo + loop entrenamiento |
| `ejecutar_modelo.py` | EjecuciÃ³n sin exploraciÃ³n (Îµ=0) |
| `test_sistema.py` | VerificaciÃ³n de configuraciÃ³n |
| `README.md` | DocumentaciÃ³n completa |
| `SIMPLIFICACIONES.md` | JustificaciÃ³n de decisiones |
| `TROUBLESHOOTING.md` | SoluciÃ³n de problemas |

## ðŸŽ“ Simplificaciones Implementadas

Para hacer el problema tratable en tiempo razonable:

1. âœ… **Pico de madera inicial** (dado por comando)
2. âœ… **Crafteo simulado** (comando `/give` automÃ¡tico)
3. âœ… **Hierro â†’ lingote automÃ¡tico** (sin horno)
4. âœ… **Mundo plano controlado** (vs mundo normal aleatorio)
5. âœ… **Sin mobs hostiles** (enfoque en exploraciÃ³n)
6. âœ… **Inventario limpiado** (cada episodio desde cero)

**Todas justificadas acadÃ©micamente** - ver `SIMPLIFICACIONES.md`

## ðŸ§  Innovaciones TÃ©cnicas

### 1. Q-Tables Modulares
En vez de una sola Q-table gigante, usamos **4 Q-tables especializadas** (una por fase). Ventajas:
- Mejor generalizaciÃ³n por fase
- Menos conflictos entre objetivos
- MÃ¡s rÃ¡pido de entrenar

### 2. ParÃ¡metros Adaptativos
Los parÃ¡metros de aprendizaje se ajustan segÃºn la fase:
```python
Fase 0 (MADERA):   Î±=0.10, Îµ=0.40  # MÃ¡s fÃ¡cil
Fase 3 (DIAMANTE): Î±=0.20, Îµ=0.25  # MÃ¡s difÃ­cil, menos exploraciÃ³n
```

### 3. Recompensas Escaladas
Las recompensas aumentan en fases avanzadas (Ã—2.0 en diamante) para priorizar completar el objetivo final sobre quedarse en fases tempranas.

### 4. Castigo por Fase Incorrecta
El agente aprende a **no** buscar madera cuando ya estÃ¡ en fase hierro (castigo -15), forzando progresiÃ³n hacia adelante.

## ðŸ“ˆ Extensiones Futuras

Una vez funciona en mundo plano:

### Corto Plazo
- [ ] Probar en mundo normal (DefaultWorldGenerator)
- [ ] Reducir cantidad de materiales (mÃ¡s realista)
- [ ] Ampliar Ã¡rea a 100Ã—100 bloques

### Mediano Plazo
- [ ] Implementar crafteo real (GUI)
- [ ] AÃ±adir mÃ¡s materiales (carbÃ³n, redstone)
- [ ] Sistema de fundiciÃ³n real (hornos)

### Largo Plazo
- [ ] Deep Q-Networks (DQN) para estados continuos
- [ ] Transfer learning a otros objetivos
- [ ] Multi-agente cooperativo

## ðŸ’¡ Lecciones Aprendidas

### âœ… QuÃ© Funciona Bien
- Q-tables modulares por fase
- Recompensas escaladas segÃºn complejidad
- Castigos fuertes por herramienta incorrecta
- Mundo plano para entrenamiento inicial

### âš ï¸ DesafÃ­os Identificados
- Agente puede atascarse en spawn (mitigado con anti-stuck)
- Fase 2â†’3 mÃ¡s difÃ­cil (hierro/diamante escasos)
- Requiere ~50+ episodios para convergencia
- Timeout crÃ­tico: muy corto â†’ falla, muy largo â†’ entrenamiento lento

### ðŸŽ¯ Mejores PrÃ¡cticas
1. Siempre empezar con mundo controlado
2. Entrenar al menos 50 episodios antes de evaluar
3. Monitorear epsilon (debe bajar a <0.1 eventualmente)
4. Guardar modelo frecuentemente (cada 10 episodios)
5. Usar semilla fija para reproducibilidad

## ðŸ”¬ Experimentos Sugeridos

### Experimento 1: Ablation Study
Entrenar versiones sin cada componente:
- Sin recompensas escaladas
- Sin Q-tables modulares
- Sin castigo por fase incorrecta
- Sin parÃ¡metros adaptativos

**HipÃ³tesis:** Todos son necesarios para buen rendimiento.

### Experimento 2: Sensibilidad a HiperparÃ¡metros
Variar Î±, Î³, Îµ en rangos:
- Î± âˆˆ [0.05, 0.3]
- Î³ âˆˆ [0.9, 0.99]
- Îµâ‚€ âˆˆ [0.2, 0.6]

**HipÃ³tesis:** Î±=0.1-0.15, Î³=0.95, Îµâ‚€=0.4 son Ã³ptimos.

### Experimento 3: Curriculum Learning
Entrenar primero solo fase 0, luego 0-1, luego 0-1-2, finalmente 0-1-2-3.

**HipÃ³tesis:** Converge mÃ¡s rÃ¡pido que aprender todo a la vez.

### Experimento 4: Transfer a Mundo Normal
1. Entrenar 100 episodios en mundo plano
2. Cargar modelo
3. Ejecutar 50 episodios en mundo normal

**HipÃ³tesis:** PolÃ­tica aprendida se transfiere parcialmente.

## ðŸ“š Referencias y Contexto

### Base TeÃ³rica
- **Q-Learning:** Watkins & Dayan (1992)
- **Hierarchical RL:** Sutton et al. (1999)
- **Minecraft RL:** MineRL Competition (NeurIPS 2019-2022)

### InspiraciÃ³n del Proyecto
Este agente implementa **aprendizaje jerÃ¡rquico** donde cada fase es un sub-objetivo. Es anÃ¡logo a:
- Robots aprendiendo tareas complejas por etapas
- Juegos RTS (recolectar â†’ construir â†’ atacar)
- NavegaciÃ³n (buscar puerta â†’ abrir â†’ atravesar)

### Diferencias con MineRL Baseline
- **MineRL:** Usa imitation learning + RL en mundo completo
- **Este proyecto:** Q-learning tabular en mundo simplificado
- **Ventaja aquÃ­:** MÃ¡s interpretable, mÃ¡s rÃ¡pido de entrenar
- **Ventaja MineRL:** MÃ¡s general, escala a tareas complejas

## ðŸ† Criterios de Ã‰xito del Proyecto

Para considerar el proyecto **exitoso**:

### MÃ­nimo Viable (60%)
- [ ] Completa fase 0 (madera) en >80% episodios
- [ ] Completa fase 1 (piedra) en >50% episodios
- [ ] Sistema de progresiÃ³n funciona correctamente

### Objetivo Principal (80%)
- [ ] Completa fase 2 (hierro) en >40% episodios
- [ ] Completa fase 3 (diamante) en >20% episodios
- [ ] Documenta curva de aprendizaje

### Excelente (100%)
- [ ] Completa las 4 fases en >70% Ãºltimos 20 episodios
- [ ] Pasos medio <400 en episodios exitosos
- [ ] Funciona tambiÃ©n en mundo normal (aunque con menor tasa)

## ðŸ“ž Contacto y Contribuciones

**Autor:** Sistema de IA  
**Fecha:** Noviembre 2025  
**VersiÃ³n:** 1.0  
**Licencia:** MIT (cÃ³digo) / CC-BY (documentaciÃ³n)

### CÃ³mo Contribuir
1. Reporta bugs en issues
2. Sugiere mejoras en discussions
3. Comparte resultados de experimentos
4. Documenta nuevas configuraciones que funcionen

---

## ðŸŽ¬ Demo RÃ¡pido

```bash
# 1. Clonar repositorio
cd "agente madera_piedra_hierro_diamante_mundo_plano"

# 2. Verificar instalaciÃ³n
python3 test_sistema.py

# 3. Entrenar 10 episodios (demo rÃ¡pido ~15 min)
python3 mundo_rl.py 10

# 4. Ejecutar modelo
python3 ejecutar_modelo.py 3

# 5. Ver estadÃ­sticas
python3 -c "
import pickle
with open('modelo_progresivo.pkl', 'rb') as f:
    m = pickle.load(f)
    print(f'Episodios: {m[\"episodios\"]}')
    print(f'Epsilon: {m[\"epsilon\"]:.4f}')
    for fase, qt in m['q_tables'].items():
        print(f'Fase {fase}: {len(qt)} estados')
"
```

**Â¡Listo para empezar!** ðŸš€

---

*Este documento es un resumen ejecutivo. Para detalles completos, consulta `README.md`.*
