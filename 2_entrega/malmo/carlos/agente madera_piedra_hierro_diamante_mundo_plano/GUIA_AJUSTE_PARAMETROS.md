# Gu√≠a R√°pida: Ajuste de Par√°metros de Entrenamiento

## üìç Ubicaci√≥n de Configuraciones

### Archivo Principal: `configuracion_entrenamiento.py`
**TODO lo que necesitas ajustar est√° aqu√≠** - No necesitas editar otros archivos.

---

## üéØ C√≥mo Modificar las Recompensas

### 1. Recompensas por Obtener Material (las M√ÅS GRANDES)
```python
# Archivo: configuracion_entrenamiento.py
# L√≠nea: ~60

RECOMPENSA_MATERIAL_OBTENIDO = {
    'madera': 200.0,      # ‚Üê Cambia aqu√≠
    'piedra': 250.0,      # ‚Üê Cambia aqu√≠
    'hierro': 300.0,      # ‚Üê Cambia aqu√≠
    'diamante': 500.0,    # ‚Üê Cambia aqu√≠
}
```

**Ejemplo**: Si quieres que el diamante valga MUCHO m√°s:
```python
RECOMPENSA_MATERIAL_OBTENIDO = {
    'madera': 200.0,
    'piedra': 250.0,
    'hierro': 300.0,
    'diamante': 1000.0,  # ¬°El doble!
}
```

---

### 2. Recompensas por Picar Correctamente
```python
# Archivo: configuracion_entrenamiento.py
# L√≠nea: ~77

RECOMPENSA_ATAQUE_CORRECTO = {
    0: 30.0,    # Fase 0: Picar madera
    1: 40.0,    # Fase 1: Picar piedra
    2: 50.0,    # Fase 2: Picar hierro
    3: 100.0,   # Fase 3: Picar diamante
}
```

**Incrementa estos valores si quieres que el agente pique m√°s agresivamente.**

---

### 3. Recompensas por Proximidad
```python
# Archivo: configuracion_entrenamiento.py
# L√≠nea: ~110

# MUY CERCA (‚â§ 2 bloques)
RECOMPENSA_OBJETIVO_MUY_CERCA = {
    0: 20.0,   # Madera muy cerca
    1: 25.0,   # Piedra muy cerca
    2: 30.0,   # Hierro muy cerca
    3: 50.0,   # Diamante muy cerca
}

# CERCA (‚â§ 4 bloques)
RECOMPENSA_OBJETIVO_CERCA = {
    0: 10.0,   # Madera cerca
    1: 12.0,   # Piedra cerca
    2: 15.0,   # Hierro cerca
    3: 25.0,   # Diamante cerca
}
```

**Incrementa si el agente tiene problemas encontrando materiales.**

---

### 4. Multiplicadores por Fase
```python
# Archivo: configuracion_entrenamiento.py
# L√≠nea: ~161

MULTIPLICADOR_FASE = {
    0: 1.0,    # MADERA: sin multiplicador
    1: 1.2,    # PIEDRA: +20%
    2: 1.5,    # HIERRO: +50%
    3: 2.0,    # DIAMANTE: +100% (todo vale el doble)
}
```

**Esto multiplica TODAS las recompensas. Fase 3 vale el doble!**

---

## üß† Par√°metros de Aprendizaje

```python
# Archivo: configuracion_entrenamiento.py
# L√≠nea: ~20

PARAMETROS_QLEARNING = {
    'alpha': 0.1,              # ‚Üê Velocidad de aprendizaje
    'gamma': 0.95,             # ‚Üê Importancia del futuro
    'epsilon_inicial': 0.4,    # ‚Üê Exploraci√≥n inicial
    'epsilon_min': 0.05,       # ‚Üê Exploraci√≥n m√≠nima
    'epsilon_decay': 0.995,    # ‚Üê Velocidad de decaimiento
}
```

### Qu√© hace cada uno:

**Alpha (Tasa de Aprendizaje)**:
- `0.1` = Aprende lento pero estable
- `0.3` = Aprende r√°pido pero puede ser inestable
- **Recomendado**: 0.1 - 0.15

**Gamma (Factor de Descuento)**:
- `0.9` = Solo importa recompensa cercana
- `0.99` = Planifica a largo plazo
- **Recomendado**: 0.95

**Epsilon Inicial (Exploraci√≥n)**:
- `0.5` = 50% acciones aleatorias (mucha exploraci√≥n)
- `0.2` = 20% acciones aleatorias (poca exploraci√≥n)
- **Recomendado**: 0.3 - 0.4

---

## ‚öôÔ∏è Configuraci√≥n del Episodio

```python
# Archivo: configuracion_entrenamiento.py
# L√≠nea: ~175

EPISODIO_CONFIG = {
    'max_pasos': 200,              # ‚Üê Pasos por episodio
    'timeout_mision_ms': 120000,   # ‚Üê Timeout (2 min)
    'delay_entre_comandos': 0.5,   # ‚Üê Velocidad
}
```

**Para entrenamientos m√°s largos**:
```python
'max_pasos': 400,              # 4 minutos por episodio
'timeout_mision_ms': 240000,   # 4 minutos timeout
```

---

## üó∫Ô∏è Configuraci√≥n del Mundo

```python
# Archivo: configuracion_entrenamiento.py
# L√≠nea: ~195

MUNDO_CONFIG = {
    'radio': 25,                      # √Årea 50x50
    'cantidad_madera': (15, 20),      # ‚Üê M√°s/menos madera
    'cantidad_piedra': (15, 20),      # ‚Üê M√°s/menos piedra
    'cantidad_hierro': (8, 12),       # ‚Üê M√°s/menos hierro
    'cantidad_diamante': (3, 5),      # ‚Üê M√°s/menos diamante
}
```

**Mundo m√°s dif√≠cil (menos materiales)**:
```python
'cantidad_madera': (8, 12),
'cantidad_piedra': (8, 12),
'cantidad_hierro': (4, 6),
'cantidad_diamante': (1, 2),
```

---

## üìä Casos de Uso Comunes

### Caso 1: "El agente no encuentra materiales"
**Soluci√≥n**: Aumentar recompensas de proximidad
```python
RECOMPENSA_OBJETIVO_MUY_CERCA = {
    0: 40.0,   # Duplicado
    1: 50.0,
    2: 60.0,
    3: 100.0,
}

RECOMPENSA_OBJETIVO_CERCA = {
    0: 20.0,   # Duplicado
    1: 24.0,
    2: 30.0,
    3: 50.0,
}
```

---

### Caso 2: "El agente aprende muy lento"
**Soluci√≥n**: Aumentar velocidad de aprendizaje
```python
PARAMETROS_QLEARNING = {
    'alpha': 0.2,              # M√°s r√°pido
    'epsilon_decay': 0.99,     # Decae m√°s r√°pido
}
```

---

### Caso 3: "El agente se distrae con materiales incorrectos"
**Soluci√≥n**: Aumentar castigos por fase incorrecta
```python
CASTIGO_FASE_INCORRECTA = {
    1: -30.0,   # Triplicado
    2: -45.0,
    3: -60.0,
}
```

---

### Caso 4: "El diamante es muy dif√≠cil de conseguir"
**Soluci√≥n**: Aumentar recompensas de fase 3
```python
RECOMPENSA_MATERIAL_OBTENIDO = {
    'diamante': 1000.0,  # ¬°Mucho m√°s!
}

MULTIPLICADOR_FASE = {
    3: 3.0,  # Triple multiplicador para fase diamante
}
```

---

## üöÄ Aplicar Cambios

1. **Edita** `configuracion_entrenamiento.py`
2. **Guarda** el archivo
3. **Ejecuta** el entrenamiento:
   ```bash
   malmoenv
   python3 mundo_rl.py 10
   ```
4. Los cambios se aplican autom√°ticamente

---

## üìù Notas Importantes

- **NO MODIFICAR OTROS ARCHIVOS**: Todo est√° en `configuracion_entrenamiento.py`
- **Backup**: Guarda una copia antes de cambios grandes
- **Probar primero**: Haz 5-10 episodios de prueba antes de entrenamientos largos
- **Logs**: Observa los mensajes para ver si las recompensas funcionan

---

## üîç D√≥nde Encontrar Qu√©

| Qu√© necesitas ajustar | Archivo | L√≠nea aprox. |
|----------------------|---------|--------------|
| Recompensas por material obtenido | `configuracion_entrenamiento.py` | ~60 |
| Recompensas por picar | `configuracion_entrenamiento.py` | ~77 |
| Castigos por herramienta incorrecta | `configuracion_entrenamiento.py` | ~85 |
| Recompensas por proximidad | `configuracion_entrenamiento.py` | ~110 |
| Par√°metros de aprendizaje | `configuracion_entrenamiento.py` | ~20 |
| Multiplicadores por fase | `configuracion_entrenamiento.py` | ~161 |
| Configuraci√≥n de episodio | `configuracion_entrenamiento.py` | ~175 |
| Configuraci√≥n de mundo | `configuracion_entrenamiento.py` | ~195 |

---

**¬øTienes dudas?** Todas las configuraciones tienen comentarios explicativos en el archivo.
