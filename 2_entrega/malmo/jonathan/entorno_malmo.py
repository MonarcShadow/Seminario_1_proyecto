"""
Wrapper del Entorno Malmo para RL
Maneja la comunicaci√≥n con Minecraft y sistema de recompensas

Autor: Sistema de IA
"""

import json
import time


class EntornoMalmo:
    """
    Entorno de Aprendizaje por Refuerzo sobre Malmo
    Proporciona interfaz similar a OpenAI Gym
    """
    
    def __init__(self, agent_host):
        """
        Par√°metros:
        -----------
        agent_host: MalmoPython.AgentHost
            Instancia del agente de Malmo
        """
        self.agent_host = agent_host
        self.world_state = None
        self.estado_previo = None
        self.arena_previa = 0
        self.posicion_previa = (0, 0, 0)
        
        # Historial para detectar loops
        self.historial_acciones = []  # √öltimas N acciones
        self.pasos_sin_movimiento = 0  # Contador de pasos sin cambio de posici√≥n
        
    def calcular_recompensa(self, obs, accion, recompensa_malmo=0.0):
        """
        Sistema de recompensas dise√±ado para guiar al agente hacia el agua
        
        RECOMPENSAS:
        +100  ‚Üí Tocar agua (de Malmo - RewardForTouchingBlockType) ¬°√âXITO!
        +10   ‚Üí Acercarse a arena (indicador de agua)
        +5    ‚Üí Descubrir √°rea nueva
        +3    ‚Üí Moverse exitosamente (cambio de posici√≥n)
        +2    ‚Üí Moverse hacia zona m√°s baja (agua suele estar abajo)
        -0.5  ‚Üí Costo por acci√≥n (de Malmo)
        +100  ‚Üí Tocar agua (de Malmo - RewardForTouchingBlockType)
        -5    ‚Üí Colisi√≥n/obst√°culo
        -20   ‚Üí Loop detectado (giros repetidos sin movimiento)
        -30   ‚Üí Atascado completamente (muchos pasos sin movimiento)
        -15   ‚Üí Alejarse de arena una vez detectada
        
        Par√°metros:
        -----------
        obs: dict
            Observaciones actuales
        accion: str
            Comando ejecutado
        recompensa_malmo: float
            Recompensa acumulada de Malmo en este paso
        
        Retorna:
        --------
        float: Recompensa calculada
        """
        # 1. USAR RECOMPENSA DE MALMO (prioridad m√°xima)
        # Si Malmo da +100, es porque toc√≥ agua ‚Üí ¬°√âXITO!
        if recompensa_malmo >= 100.0:
            print("üéâ ¬°AGUA TOCADA! Recompensa de Malmo detectada")
            return recompensa_malmo
        
        # Recompensa base incluye el costo por acci√≥n de Malmo (-0.5)
        recompensa = recompensa_malmo  # Ya incluye -0.5 por comando
        
        # 2. SE√ëAL DE PROXIMIDAD: Arena indica cercan√≠a a agua
        grid = obs.get("near5x3x5", [])
        arena_actual = sum(1 for bloque in grid if bloque == "sand")
        
        # Eliminar detecci√≥n manual de agua (ahora Malmo lo maneja)
        # 3. SE√ëAL DE PROXIMIDAD: Arena indica cercan√≠a a agua
        
        if arena_actual > self.arena_previa:
            # Encontramos M√ÅS arena ‚Üí vamos bien
            recompensa += 10.0
        elif arena_actual > 0 and arena_actual < self.arena_previa:
            # Nos ALEJAMOS de arena ‚Üí penalizaci√≥n
            recompensa -= 15.0
        elif arena_actual > 0:
            # Mantenemos arena visible ‚Üí peque√±o bonus
            recompensa += 2.0
        
        self.arena_previa = arena_actual
        
        # 3. DETECTAR MOVIMIENTO REAL
        x = obs.get("XPos", 0)
        y = obs.get("YPos", 64)
        z = obs.get("ZPos", 0)
        posicion_actual = (round(x, 1), round(y, 1), round(z, 1))
        
        # Calcular si hubo movimiento significativo
        distancia = ((posicion_actual[0] - self.posicion_previa[0])**2 + 
                    (posicion_actual[2] - self.posicion_previa[2])**2)**0.5
        
        if distancia > 0.3:  # Se movi√≥ al menos 0.3 bloques
            # ‚úÖ MOVIMIENTO EXITOSO
            recompensa += 3.0
            self.pasos_sin_movimiento = 0
            
            # Bonus por explorar hacia abajo (agua suele estar en Y bajo)
            if y < self.posicion_previa[1]:
                recompensa += 2.0
        else:
            # ‚ùå NO SE MOVI√ì
            self.pasos_sin_movimiento += 1
            
            # Agregar acci√≥n al historial
            self.historial_acciones.append(accion)
            if len(self.historial_acciones) > 10:
                self.historial_acciones.pop(0)
            
            # DETECTAR LOOP: Si lleva muchos giros consecutivos
            if self.pasos_sin_movimiento > 3:
                # Revisar si est√° alternando entre giros
                ultimas_3 = self.historial_acciones[-3:]
                if all("turn" in a for a in ultimas_3):
                    recompensa -= 20.0  # CASTIGO FUERTE por loop de giros
            
            # ATASCADO COMPLETAMENTE
            if self.pasos_sin_movimiento > 8:
                recompensa -= 30.0  # CASTIGO MUY FUERTE
                
            # Penalizaci√≥n progresiva por quedarse quieto
            recompensa -= (2.0 * self.pasos_sin_movimiento)
        
        self.posicion_previa = posicion_actual
        
        # 4. INCENTIVAR INTENTOS DE MOVIMIENTO despu√©s de giros
        # Si acaba de girar, la siguiente acci√≥n DEBE ser move o jumpmove
        if len(self.historial_acciones) >= 2:
            ultima = self.historial_acciones[-1] if len(self.historial_acciones) > 0 else ""
            penultima = self.historial_acciones[-2] if len(self.historial_acciones) > 1 else ""
            
            # Si gir√≥ en la acci√≥n anterior y ahora intenta moverse ‚Üí BONUS
            if "turn" in penultima and ("move" in accion or "jumpmove" in accion):
                recompensa += 5.0  # Recompensar intentar avanzar despu√©s de girar
        
        # 5. PENALIZACI√ìN POR OBST√ÅCULOS (solo si intent√≥ moverse y no lo logr√≥)
        if ("move" in accion or "jumpmove" in accion) and distancia < 0.1:
            # Intent√≥ moverse pero no se movi√≥ ‚Üí hay obst√°culo
            recompensa -= 5.0
        
        # 6. RECOMPENSA POR VARIEDAD DE BLOQUES (exploraci√≥n)
        tipos_unicos = len(set(grid))
        if tipos_unicos > 3:
            recompensa += 1.0  # Bonus por descubrir √°rea diversa
        
        return recompensa
    
    def obtener_observacion(self):
        """
        Extrae observaciones del world_state actual
        
        Retorna:
        --------
        dict: Observaciones parseadas o None si no hay datos
        """
        if self.world_state is None:
            return None
        
        if self.world_state.number_of_observations_since_last_state > 0:
            obs_text = self.world_state.observations[-1].text
            obs = json.loads(obs_text)
            return obs
        
        return None
    
    def ejecutar_accion(self, comando, duracion=0.5):
        """
        Env√≠a comando al agente y espera respuesta
        
        Par√°metros:
        -----------
        comando: str
            Comando de Malmo (ej: "move 1", "turn 1")
        duracion: float
            Tiempo de espera despu√©s del comando
        """
        self.agent_host.sendCommand(comando)
        time.sleep(duracion)
        
        # Detener movimiento continuo
        if "move" in comando:
            time.sleep(0.1)
            # No detener expl√≠citamente para comandos discretos
    
    def actualizar_world_state(self):
        """
        Actualiza el world_state obteniendo el estado actual de Malmo
        
        Retorna:
        --------
        bool: True si la misi√≥n sigue corriendo, False si termin√≥
        """
        self.world_state = self.agent_host.getWorldState()
        
        # Verificar errores
        for error in self.world_state.errors:
            print(f"‚ùå Error Malmo: {error.text}")
        
        return self.world_state.is_mission_running
    
    def verificar_agua_encontrada(self, obs, recompensa_malmo=0.0):
        """
        Verifica si el agente encontr√≥ agua usando recompensas de Malmo
        
        Par√°metros:
        -----------
        obs: dict
            Observaciones actuales
        recompensa_malmo: float
            Recompensa de Malmo (si es >= 100, toc√≥ agua)
        
        Retorna:
        --------
        bool: True si toc√≥ agua (recompensa de Malmo >= 100)
        """
        # M√âTODO PRINCIPAL: Usar recompensa de Malmo (m√°s confiable)
        if recompensa_malmo >= 100.0:
            print("üíß ¬°AGUA TOCADA! (confirmado por RewardForTouchingBlockType)")
            return True
        
        # M√âTODO ALTERNATIVO: Revisar grid (solo para monitoreo, no decisi√≥n)
        if obs is None:
            return False
        
        grid = obs.get("near5x3x5", [])
        agua_en_grid = any(
            bloque in ["water", "flowing_water", "stationary_water"]
            for bloque in grid
        )
        
        if agua_en_grid:
            print("üëÄ Agua visible en grid (cerca pero no tocada a√∫n)")
        
        return False  # Solo retorna True si TOC√ì agua (recompensa de Malmo)
    
    def reset(self):
        """
        Resetea el entorno (para nuevos episodios)
        """
        self.estado_previo = None
        self.arena_previa = 0
        self.posicion_previa = (0, 0, 0)
        self.historial_acciones = []
        self.pasos_sin_movimiento = 0
